{
    "contents" : "############################################################\n### Code for module 5: the basic Nmix model for closed metapopulations\n############################################################\n# 5.3. Simulation of data and first analysis using unmarked\n\n# Create a covariate called vegHt\nnSites <- 100                # Also called M\nset.seed(443)                # so that we all get the same values of vegHt\nvegHt <- sort(runif(nSites, 1, 3)) # uniform from 1 to 3, sort for graph convenience\n\n# Suppose that expected population size increases with vegHt\n# The relationship is described by an intercept of -3 and\n#    a slope parameter of 2 on the log scale\nlambda <- exp(-3 + 2*vegHt)\n\n# Now we go to 100 sites and observe the # of individuals (perfectly)\nN <- rpois(nSites, lambda)\n\n# We can fit a model that relates abundance to vegHt using the glm() function\n#  with \"family=Poisson\":\n\nsummary(fm.glm1 <- glm(N ~ vegHt, family=poisson))\n\n# Do some analysis of the results\nplot(vegHt, N, xlab=\"Vegetation height\", ylab=\"Abundance (N)\")\nglm1.est <- coef(fm.glm1)\nplot(function(x) exp(-3 + 2*x), 1, 3, add=TRUE, lwd=3)\nplot(function(x) exp(glm1.est[1] + glm1.est[2]*x), 1, 3, add=TRUE,\n     lwd=3, col=\"blue\")\nlegend(1, 20, c(\"Truth\", \"Estimate\"), col=c(\"black\", \"blue\"), lty=1,\n       lwd=3)\n\nnVisits <- 3\np <- 0.6\nC <- matrix(NA, nSites, nVisits)\nfor(i in 1:nSites) {\n    C[i,] <- rbinom(nVisits, N[i], p)\n}\n\n# Look at the data\ncbind(N=N, C1=C[,1], C2=C[,2], C3=C[,3])\n\n# Load library, format data and summarize\nlibrary(unmarked)\numf <- unmarkedFramePCount(y=C, siteCovs=as.data.frame(vegHt))\nsummary(umf)\n\n# Fit a model and extract estimates\n# Detection covariates follow first tilde, then come abundance covariates\n# Also time model fit and monitor convergence\nsystem.time(summary(fm.nmix1 <- pcount(~1 ~vegHt, data=umf, control=list(trace=TRUE, REPORT=1))))\n\n# May consider other abundance models: NB = negative binomial, ZIP = zero-inflated Poisson (currently no others available)\n# Compare with AIC\nfm.nmix2<-  pcount(~1 ~vegHt, data=umf,mixture=\"NB\", control=list(trace=TRUE, REPORT=1))\nfm.nmix3<-  pcount(~1 ~vegHt, data=umf,mixture=\"ZIP\", control=list(trace=TRUE, REPORT=1))\ncbind(AIC.P=fm.nmix1@AIC, AIC.NB=fm.nmix2@AIC, AIC.ZIP=fm.nmix3@AIC)\n        AIC.P   AIC.NB  AIC.ZIP\n[1,] 870.3834 872.3845 872.3414\n\n\n# Note, estimates of detection coefficients are on the logit-scale\n# When there are no covariates, we can back-transform using:\nbeta1 <- coef(fm.nmix1) \nexp(beta1[3]) / (1+exp(beta1[3]))   # or\nplogis(beta1[3])                    # or\nbackTransform(fm.nmix1, type=\"det\") # estimate with SE\n\n# When covariates are present we can do something like\nplot(function(x) exp(beta1[1] + beta1[2]*x), 1, 3,\n     xlab=\"vegetation height\", ylab=\"Expected Abundance\")\n\n# Or suppose you want predictions for new values of vegHt, say 1.2 and 3.1\nnewdat <- data.frame(vegHt=c(1.2, 3.1))\npredict(fm.nmix1, type=\"state\", newdata=newdat)\n> predict(fm.nmix1, type=\"state\", newdata=newdat)\n   Predicted         SE      lower      upper\n1  0.6281663 0.09739854  0.4635472  0.8512464\n2 24.5194200 2.70937154 19.7448241 30.4485852\n\nranef(fm.nmix1)\n\n## 5.4. Analysis of Alder flycatcher point count data using unmarked\n# PART 1: Set-up the data for analysis\n# -------------------------- Format data ---------------------------------\n# This a subset of point-count data from Chandler et al. (Auk 2009)\n# alfl is Alder Flycatcher (Empidonax alnorum)\n\n# Import data and check structure\n#alfl.data <- read.csv(\"alfl05.csv\", row.names=1)\nalfl.data <- read.csv(\"http://sites.google.com/site/unmarkedinfo/home/webinars/2012-january/data/alfl05.csv?attredirects=0&d=1\", row.names=1)\nstr(alfl.data)\n\n# Pull out count matrix \nalfl.y <- alfl.data[,c(\"alfl1\", \"alfl2\", \"alfl3\")]\n\n# Standardize site-covariates\nwoody.mean <- mean(alfl.data$woody)\nwoody.sd <- sd(alfl.data$woody)\nwoody.z <- (alfl.data$woody-woody.mean)/woody.sd\nstruct.mean <- mean(alfl.data$struct)\nstruct.sd <- sd(alfl.data$struct)\nstruct.z <- (alfl.data$struct-struct.mean)/struct.sd\n\n# Load and create unmarkedFrame \n# Note formatting of unmarked data frame\nlibrary(unmarked)\nalfl.umf <- unmarkedFramePCount(y=alfl.y,\n    siteCovs=data.frame(woody=woody.z, struct=struct.z),\n    obsCovs=list(time=alfl.data[,c(\"time.1\", \"time.2\", \"time.3\")],\n                 date=alfl.data[,c(\"date.1\", \"date.2\", \"date.3\")]))\nsummary(alfl.umf)\n\n# Here's an easy way to standardize covariates after making the UMF\nobsCovs(alfl.umf) <- scale(obsCovs(alfl.umf))\nsummary(alfl.umf)\n\n\n\n# PART 2: Fit some models\n# -------------------------- Model fitting  -----------------------------\n(fm1 <-  pcount(~1 ~1, alfl.umf))\n backTransform(fm1, type=\"state\")\n backTransform(fm1, type=\"det\")\n(fm2 <- pcount(~date+time ~1, alfl.umf))\n(fm3 <- pcount(~date+time ~woody, alfl.umf))\n(fm4 <- pcount(~date+time ~woody+struct, alfl.umf))\n(fm5 <- pcount(~date+time ~1, alfl.umf,mixture=\"NB\"))\n(fm6 <- pcount(~date+time ~1, alfl.umf,mixture=\"ZIP\"))\n(fm7 <- pcount(~date+time ~woody,alfl.umf,mixture=\"ZIP\"))\n(fm8 <- pcount(~date+time ~struct,alfl.umf,mixture=\"ZIP\"))\n(fm9 <- pcount(~date+time ~woody+struct, alfl.umf,mixture=\"ZIP\"))\n(fm10<- pcount(~date+time ~woody+struct, alfl.umf,mixture=\"NB\"))\n\n# -------------------------- Model selection -----------------------------\n# Put the fitted models in a \"fitList\"\nfms <- fitList(\"lam(.)p(.)\"                    = fm1,\n               \"lam(.)p(date+time)\"            = fm2,\n               \"lam(woody)p(date+time)\"        = fm3,\n               \"lam(woody+struct)p(date+time)\" = fm4,\n               \"lam(.)p(date+time)NB\"          = fm5,\n               \"lam(.)p(date+time)ZIP\"         = fm6,\n               \"lam(woody)p(date+time)ZIP\"     = fm7,\n               \"lam(struct)p(date+time)ZIP\"    = fm8,\n               \"lam(woody+struct)p(date+time)ZIP\"=fm9,\n               \"lam(woody+struct)p(date+time)NB\" =fm10)\n\n# Rank them by AIC\n(ms <- modSel(fms))\n\n# Table with everything you could possibly need\ncoef(ms)\ntoExport <- as(ms, \"data.frame\")\nstr(toExport)               # See what?s in there\n\n\n\n# PART 3: Do some analysis of the results\n# ---------------------------- Prediction --------------------------------\n# Expected detection probability as function of time of day\n# We standardized \"time\", so we predict over range of values on that scale\n# We must fix \"date\" at some arbitrary value (let's use the mean)\nnewData1 <- data.frame(time=seq(-2.08, 1.86, by=0.1), date=0)\nE.p <- predict(fm4, type=\"det\", newdata=newData1, appendData=TRUE)\nhead(E.p)\n\n# Plot it\n# Expected detection over time of day\npar(mfrow=c(1,2))\nplot(Predicted ~ time, E.p, type=\"l\", ylim=c(0,1), xlab=\"time of day (standardized)\", ylab=\"Expected detection probability\", col = \"blue\", lwd = 3)\nlines(lower ~ time, E.p, type=\"l\", col=gray(0.5))\nlines(upper ~ time, E.p, type=\"l\", col=gray(0.5))\n\n# Expected abundance over range of \"woody\"\nnewData2 <- data.frame(woody=seq(-1.6, 2.38,,50),struct=seq(-1.8,3.2,,50))\nE.N <- predict(fm4, type=\"state\", newdata=newData2, appendData=TRUE)\nhead(E.N)\n\n# Plot predictions with 95% CI\nplot(Predicted ~ woody, E.N, type=\"l\", ylim=c(-.1,max(E.N$Predicted)), xlab=\"woody vegetation (standardized)\", ylab=\"Expected abundance, E[N]\", col = \"blue\", lwd = 3)\nlines(lower ~ woody, E.N, type=\"l\", col=gray(0.5))\nlines(upper ~ woody, E.N, type=\"l\", col=gray(0.5))\n\n# Plot it again, but this time convert the x-axis back to original scale\npar(mfrow=c(1,1))\nplot(Predicted ~ woody, E.N, type=\"l\", ylim=c(-.1,max(E.N$Predicted)),\n     xlab=\"Percent cover - woody vegetation\",\n     ylab=\"Expected abundance, E[N]\",\n     xaxt=\"n\")\nxticks <- -1:2\nxlabs <- xticks*woody.sd + woody.mean\naxis(1, at=xticks, labels=round(xlabs, 1))\nlines(lower ~ woody, E.N, type=\"l\", col=gray(0.5))\nlines(upper ~ woody, E.N, type=\"l\", col=gray(0.5))\n\n# Get predictions for covariates in the observation model on natural scale\n# More step-by-step approach\n# Look at range, mean and sd of covariate\n(range.time <- range(alfl.data[,7:9]))\n(mean.time <- mean(as.matrix(alfl.data[,7:9])))\n(sd.time <- sd(c(as.matrix(alfl.data[,7:9]))))\n(range.date <- range(alfl.data[,10:12]))\n(mean.date <- mean(as.matrix(alfl.data[,10:12])))\n(sd.date <- sd(c(as.matrix(alfl.data[,10:12]))))\n\n# Create new covariate for prediction and scale identically\noriginal.time.pred <- seq(5,10,,100)\noriginal.date.pred <- seq(1,54,,100)\ntime.pred <- (original.time.pred - mean.time) / sd.time\ndate.pred <- (original.date.pred - mean.date) / sd.date\n\n# Compute predictions for both covariates, keeping other constant\nnewData1 <- data.frame(time=time.pred, date=0)\nEp1 <- predict(fm4, type=\"det\", newdata=newData1, appendData=TRUE)\nhead(Ep1)\nnewData2 <- data.frame(time=0, date=date.pred)\nEp2 <- predict(fm4, type=\"det\", newdata=newData2, appendData=TRUE)\nhead(Ep2)\n\n# Plot against covariate on natural scale\npar(mfrow=c(1,2))\nplot(Ep1$Predicted ~ original.time.pred, type=\"l\", ylim=c(0,1), xlab=\"Time of day\", ylab=\"Expected detection probability\", main = \"Effect of time of day\", col = \"blue\", lwd = 3)\nlines(Ep1$lower ~ original.time.pred, type=\"l\", col=gray(0.5))\nlines(Ep1$upper ~ original.time.pred, type=\"l\", col=gray(0.5))\n\nplot(Ep2$Predicted ~ original.date.pred, type=\"l\", ylim=c(0,1), xlab=\"Date\", ylab=\"Expected detection probability\", main = \"Effect of date\", col = \"blue\", lwd = 3)\nlines(Ep2$lower ~ original.date.pred, type=\"l\", col=gray(0.5))\nlines(Ep2$upper ~ original.date.pred, type=\"l\", col=gray(0.5))\n\n# ---------------------------- Goddess of fit -------------------------------\n### Here's an example of a bootstrap GoF analysis.\n### Best model is in \"fm4\" object\n\n# Function returning three fit-statistics.\nfitstats <- function(fm) {\n    observed <- getY(fm@data)\n    expected <- fitted(fm)\n    resids <- residuals(fm)\n    sse <- sum(resids^2)                                 # Sums of squares\n    chisq <- sum((observed - expected)^2 / expected)     # Chisquared\n    freeTuke <- sum((sqrt(observed) - sqrt(expected))^2) # Freeman-Tukey\n    out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)\n    return(out)\n    }\n\n(pb <- parboot(fm4, fitstats, nsim=100, report=1))\nprint(pb)\nplot(pb)\n\n## Now let?s bootstrap a summary statistic \n## This is not too meaningful right now but we will do a similar thing \n## later in a more relevant context\n\n# Total population size (derived parameter) over 50 surveyed sites\nNhat <- function(fm) {\n    N <- sum(predict(fm, type=\"state\")$Predicted, na.rm=TRUE)\n    }\n    \n(pb.N <- parboot(fm4, Nhat, nsim=25, report=5))\nplot(pb.N)\n\n# Here's an example of model-averaging predictions\n# See pg 150, section 4.2.1, of Burnham and Anderson (2002)\n# This might be worthwhile since fm3 and fm4 had similar support\nnewData3 <- data.frame(woody=seq(-1.6, 2.38,,50), struct=seq(-1.8,3.2,,50))\n\n## averages over _all_ models in the fit list \"fms\":\nE.N.bar <- predict(fms, type=\"state\", newdata=newData3, appendData=TRUE)\nhead(E.N.bar)\n\n# Plot it\nplot(Predicted ~ woody, E.N.bar, type=\"l\", ylim=c(-0.1, max(E.N$Predicted)),\n     xlab=\"Percent cover - woody vegetation\",\n     ylab=\"Expected abundance, E[N]\",\n     xaxt=\"n\")\nxticks <- -1:2\nxlabs <- xticks*woody.sd + woody.mean\naxis(1, at=xticks, labels=round(xlabs, 1))\nlines(lower ~ woody, E.N.bar, type=\"l\", col=gray(0.5))\nlines(upper ~ woody, E.N.bar, type=\"l\", col=gray(0.5))\n\n\n\n## 5.5. Analysis of Swiss willow tits with unmarked\n\n# PART 1: Set-up of analysis\n# Read in some data from the Swiss MHB survey (Swiss equivalent of BBS)\nmhbdata <- read.csv(\"http://sites.google.com/site/unmarkedinfo/home/webinars/2012-january/data/wtmatrix.csv?attredirects=0&d=1\")\n#mhbdata<-read.csv(\"wtmatrix.csv\")\nmhbdata[1:10,]\n\nlibrary(\"unmarked\")\n\nmhb.y<-mhbdata[,c(\"c.1\",\"c.2\",\"c.3\")]\nmhbdata[,\"length\"]<-1/mhbdata[,\"length\"]\nmhb.umf<-unmarkedFramePCount(y=mhb.y,\nsiteCovs=data.frame(elev=mhbdata[,\"elev\"],forest=mhbdata[,\"forest\"],length=mhbdata[,\"length\"]),\nobsCovs=list(duration=mhbdata[,c(\"dur.1\",\"dur.2\",\"dur.3\")],\n             day = mhbdata[,c(\"day.1\",\"day.2\",\"day.3\")])  )\n# this is extremely handy:\nobsCovs(mhb.umf)<- scale(obsCovs(mhb.umf))\n#\n# NOTE: Do not standardize 1/length because we are using 1/length\n# for a specific reason\nsiteCovs(mhb.umf)$forest<-scale(siteCovs(mhb.umf)$forest)\nsiteCovs(mhb.umf)$elev<-scale(siteCovs(mhb.umf)$elev)\nstr(mhb.umf)\n\n\n## PART 2: Fit some models\n# ------ Fit Poisson N-mixture model to MHB data and do model selection -----#\n## Do simplistic two-step model selection with p first\nfm01 <- pcount(~day ~1, mhb.umf)\nfm02 <- pcount(~day+I(day^2) ~1, mhb.umf)\nfm1<- pcount(~1 ~1, mhb.umf)\n\n# Put three fitted models in a \"fitList\" and rank them by AIC\nfms <- fitList(\"lam(.)p(.)\"             = fm1,\n               \"lam(.)p(day)\"           = fm01,\n               \"lam(.)p(day+day2)\"      = fm02)\n(ms <- modSel(fms))\n# So no evidence for seasonal effect on detection within simplest model for lambda\n\n# Go on modeling abundance part of model with p=constant\nfm2 <- pcount(~1 ~elev,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm3<- pcount(~1 ~forest,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm4<- pcount(~1 ~length,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm5<- pcount(~1 ~forest+elev,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm6<- pcount(~1 ~forest+length,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm7<- pcount(~1 ~elev+length,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm8<- pcount(~1 ~forest+elev+length,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm9<- pcount(~1 ~elev + I(elev^2),mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm10<- pcount(~1 ~forest+elev+I(elev^2) + length,mhb.umf, control=list(trace=TRUE, REPORT=1))\nfm11<- pcount(~1 ~forest+elev+I(elev^2)+I(elev^3) + length,mhb.umf, control=list(trace=TRUE, REPORT=1))\n\n# Put fitted models in a \"fitList\" and rank them by AIC\nmspart1<- fitList(\n\"lam(.)p(.)\"                         = fm1,\n\"lam(elev)p(.)\"                      = fm2,\n\"lam(forest)p(.)\"                    = fm3,\n\"lam(length)p(.)\"                    = fm4,\n\"lam(forest+elev)p(.)\"               = fm5,\n\"lam(forest+length)p(.)\"             = fm6,\n\"lam(elev+length)p(.)\"               = fm7,\n\"lam(forest+elev+length)p(.)\"        = fm8,\n\"lam(elev + elev^2)p(.)\"             = fm9,\n\"lam(forest+elev+elev^2+length)p(.)\" = fm10,\n\"lam(forest+elev+elev^2+elev^3+length)p(.)\" = fm11)\n(ms1 <- modSel(mspart1))\n\nprint(coef(ms1), dig = 2)\n\n# Revisit date effects within current best model and compare with AIC\nsystem.time(fm12 <- pcount(~day ~forest+elev+I(elev^2) +I(elev^3) + length,\n     mhb.umf,control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm13 <- pcount(~day + I(day^2)~forest+elev+I(elev^2)+ I(elev^3) +\n     length, mhb.umf, control=list(trace=TRUE, REPORT=1)))\nmspart2<- fitList(\n\"lam(forest+elev+elev^2+elev^3+length)p(.)\"           = fm10,\n\"lam(forest+elev+elev^2+elev^3+length)p(day)\"         = fm12,\n\"lam(forest+elev+elev^2+elev^3+length)p(day+day2)\"    = fm13)\n(ms2 <- modSel(mspart2))\n# Now good evidence for seasonal effects on p\n\n# Table with everything you could possibly need\ncoef(ms2)\n(toExport <- as(ms2, \"data.frame\"))\n\n\n\n### PART 3: Do some analysis of the results\n# --------- Goodness of fit of Poisson models -----------#\n## Does this model fit worth a darn?\n# Function returning three fit-statistics.\n# NOTE: na.rm=TRUE !!!!!!\nfitstats <- function(fm) {\n    observed <- getY(fm@data)\n    expected <- fitted(fm)\n    resids <- residuals(fm)\n    sse <- sum(resids^2,na.rm=TRUE)\n    chisq <- sum((observed - expected)^2 / expected,na.rm=TRUE)\n    freeTuke <- sum((sqrt(observed) - sqrt(expected))^2,na.rm=TRUE)\n    out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)\n    return(out)\n    }\n\n# (pb.mhb <- parboot(fm13, fitstats, nsim=100, report=1)) # this takes a while\n(pb.mhb <- parboot(fm13, fitstats, nsim=10, report=1)) # cheaper version\nplot(pb.mhb)\n# Ouch ! Model does not fit!\n\n\n# What should we do ? \n# Three choices:\n# 1. We can expand this model or tinker with components of it (NB, ZIP)\n#   [takes a long time to run these models]\n# 2. We can seek out an implausible data-generating model that fits.\n#   [might satisfy referee to have good p-value]\n# 3. We can proceed anyway.\n\n\n# Try choice 1:\n# --- Fit NegBin N-mixture models to MHB data and do model selection ----#\n# Try same models with NegBin N: accounts for extra-Poisson dispersion\nsystem.time(fm20 <- pcount(~1 ~elev, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm21<- pcount(~1 ~forest, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm22<- pcount(~1 ~length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm23<- pcount(~1 ~forest+elev, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm24<- pcount(~1 ~forest+length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm25<- pcount(~1 ~elev+length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm26<- pcount(~1 ~forest+elev+length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm27<- pcount(~1 ~elev + I(elev^2), mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm28<- pcount(~1 ~forest+elev+I(elev^2) + length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm29<- pcount(~1 ~forest+elev+I(elev^2)+I(elev^3) + length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm30<- pcount(~day ~forest+elev+I(elev^2)+I(elev^3) + length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\nsystem.time(fm31<- pcount(~day + I(day^2) ~forest+elev+I(elev^2)+I(elev^3) + length, mixture = \"NB\", mhb.umf, control=list(trace=TRUE, REPORT=1)))\n\n\n# Put fitted models in a \"fitList\" and rank them by AIC\n# Add best Poisson model for comparison\nmspart3<- fitList(\n\"lam(P, forest+elev+elev^2+elev^3+length)p(day+day^2)\" = fm13,\n\"lam(NB, elev)p(.)\"                      = fm20,\n\"lam(NB, forest)p(.)\"                    = fm21,\n\"lam(NB, length)p(.)\"                    = fm22,\n\"lam(NB, forest+elev)p(.)\"               = fm23,\n\"lam(NB, forest+length)p(.)\"             = fm24,\n\"lam(NB, elev+length)p(.)\"               = fm25,\n\"lam(NB, forest+elev+length)p(.)\"        = fm26,\n\"lam(NB, elev + elev^2)p(.)\"             = fm27,\n\"lam(NB, forest+elev+elev^2+length)p(.)\" = fm28,\n\"lam(NB, forest+elev+elev^2+elev^3+length)p(.)\" = fm29,\n\"lam(NB, forest+elev+elev^2+elev^3+length)p(day)\" = fm30,\n\"lam(NB, forest+elev+elev^2+elev^3+length)p(day+day^2)\" = fm31)\n(ms3 <- modSel(mspart3))\n\n# --- Do parboot GOF test for current AIC best NegBin model -----------#\n (pb.mhb <- parboot(fm31, fitstats, nsim=10, report=1)) # cheap version\nplot(pb.mhb)\n# Looks much better now. Use this model for inference about willow tit distribution and abundance in Switzerland\n\n\n# ---------------------- Do 1D predictions ----------------------------#\n## Now we want to use these parameter estimates to do a couple things\n## Predict response of E[N] vs. elevation -- find optimal elevation\n## make an abundance map over Switzerland \n\nrange(mhbdata[,\"elev\"])\n> range(mhbdata[,\"elev\"])\n[1]  250 2750\n(elev.mean<- attr(siteCovs(mhb.umf)$elev,\"scaled:center\"))\n(elev.sd <- attr(siteCovs(mhb.umf)$elev,\"scaled:scale\"))\n\n# remember length = 0 is saturation sampling because length = 1/L\n# Create covariate values for prediction and then scale them identically\noriginal.pred.elev <- seq(250,2750,,1000)\npred.elev <- (original.pred.elev - elev.mean) / elev.sd\nnewData<- data.frame(elev=pred.elev, forest=0, length=0)\npred<-predict(fm31, type=\"state\", newdata=newData, appendData=TRUE)\nhead(pred)\n\nplot(Predicted ~ original.pred.elev, pred,type=\"l\",xlab=\"Elevation (not standardized)\", ylab=\"Expected # territories\",ylim=c(0,100), lwd = 2)\nlines(lower ~ original.pred.elev, pred,type=\"l\",col=\"red\", lwd = 2)\nlines(upper ~ original.pred.elev, pred,type=\"l\",col=\"red\", lwd = 2)\n# Note considerable uncertainty with NB state model\n\n# what is primo elevation for the willow tit?\n\n# For quadratic response could use calculus\n#   quadratic response:\n#      y = a + b*x + c*x2\n#   differentiate and set to 0:\n#      dy/dx = b + 2*c*x = 0\n#   solve\n#      xopt = -b/(2*c)\n\n# Empirical answer for any model of covariate\n(original.pred.elev[pred$Predicted == max(pred$Predicted)])\n[1] 1891.642\n\n\n### PART 3c:\n### SPATIAL ANALYSIS/PREDICTION\n### Now lets make some really cool spatial predictions (aka MAPS)\n\nlandscape <- read.csv(\"http://sites.google.com/site/unmarkedinfo/home/webinars/2012-january/data/Swiss_landscape.csv?attredirects=0&d=1\")\n# landscape<-read.csv(\"Swiss_landscape.csv\")\nhead(landscape)\n## Note integer coordinates - row/column ids\n\ngelev<- landscape[,\"medel\"]   # median elevation of quadrat\ngforest<-landscape[,\"forest\"]\ngrid<-landscape[,c(\"x\",\"y\")]\n\n# lets plot these variables to see how they look\n#\n# two options: (1) use my simpleton spatial.plot function\n#              (2) stuff the data into a matrix and use image()\n#\n# grab utility functions including spatial.plot\n\nsource(\"http://sites.google.com/site/unmarkedinfo/home/webinars/2012-january/data/utils.R?attredirects=0&d=1\")\n\n# par(mar=c(3,3,3,5),mfrow=c(2,1))\npar(mar=c(3,3,3,6))\nspatial.plot(grid,gelev)\ntext(500, 290, labels = \"Elevation\")\nspatial.plot(grid,gforest)\ntext(500, 290, labels = \"Forest cover\")\n# this is cool\n\n\n# ------------------ Do 2D predictions (maps) ----------------------------#\nlibrary(sp)\n# Get coordinates and rescale from km to m\ncoordCH <- matrix(cbind(landscape$x + 0.5, landscape$y + 0.5), ncol = 2)\nxcor <- coordCH[,1] * 1000\nycor <- coordCH[,2] * 1000\n\n# Get predictions for each pixel of Switzerland\n# First scale values of elevation analogously to analysis\npelev <- (landscape$medel-elev.mean)/elev.sd\nforest.mean <- mean(mhbdata[,\"forest\"])\nforest.sd <- sd(mhbdata[,\"forest\"])\npforest <- (landscape$forest-forest.mean)/forest.sd\n### new<- data.frame(elev=pelev,forest=pforest,length=0)\n## pred<-predict(fm31,type=\"state\",newdata=new,appendData=TRUE)\n###\n### this would destroy your computer -> bug in unmarked\n###\n### instead we have to do this the old-fashioned way:\n### look at col names to figure order of parameters\n\npar(mfrow=c(1,1))\nbetavec<-coef(fm31)[1:5]\nXg<-cbind(rep(1,length(pelev)),pforest,pelev,pelev^2, pelev^3)\npred<-exp(Xg%*%(betavec))\n\n# Define a new dataframe with coordinates and outcome to be plotted \nPARAM <- data.frame(x = xcor, y = ycor, z = pred)\n\n# Convert the dataframe first into a SpatialPointsDataFrame and then into a SpatialPixelsDataFrame\ncoordinates(PARAM)<- ~x+y\ngridded(PARAM) <- TRUE\n\n# Plot the map using custom color palette\n# mapPalette <- colorRampPalette(c(\"grey\", \"lightgreen\", \"darkgreen\"))\nmapPalette <- colorRampPalette(c(\"grey\", \"yellow\", \"orange\", \"red\"))\nspplot(PARAM, col.regions = mapPalette(100), main = \"Expected willow tit density\")\n# Perhaps too high ? check with literature\n\n\n# Next get total population size of breeding territories (derived parameter). \nN<- sum(pred)\nprint(N)\n\n# Bootstrap the SE\n# note: not using predict() here but doing calculation by hand\nNhat <- function(fm) {\n   betavec<-coef(fm)[1:5]\n   Xg<-cbind(rep(1,length(pelev)),pforest,pelev,pelev^2, pelev^3)\n   pred<-exp(Xg%*%(betavec))\n   N<-sum(pred)\n   N\n   }\n\nestimate.of.territories<-Nhat(fm31)\n(pb.N <- parboot(fm31, Nhat, nsim=100, report=1))  # Would have to do many more times\nplot(pb.N)\nbs.sample <- pb.N@t.star\nsummary(bs.sample)\nquantile(bs.sample, prob = c(0.025, 0.975))\n> quantile(bs.sample, prob = c(0.025, 0.975))\n     2.5%     97.5% \n 132110.9 1193516.1                # 95% confidence interval\n# Much uncertainty in estimate !\n\n     \n############################################################################################\n## 5.6. Analysis of the N-mixture model with BUGS/JAGS and introduction to Bayesian p-values\n# Bundle data\nwin.data <- list(C = C, R = nrow(C), T = ncol(C), vegHt = vegHt)\n\n\n# Specify model in BUGS language\nsink(\"Nmix.txt\")\ncat(\"\nmodel {\n\n# Priors\nalpha0 ~ dunif(-10, 10)\nalpha1 ~ dunif(-10, 10)\np ~ dunif(0, 1)\n\n# Likelihood\n# Ecological model for true abundance\nfor (i in 1:R){\n   N[i] ~ dpois(lambda[i])\n   log(lambda[i]) <- alpha0 + alpha1 * vegHt[i]\n\n   # Observation model for replicated counts\n   for (j in 1:T){\n      C[i,j] ~ dbin(p, N[i])\n\n         # Assess model fit using Chi-squared discrepancy\n         # Compute fit statistic E for observed data\n         eval[i,j] <- p * N[i]   \t# Expected values\n         E[i,j] <- pow((C[i,j] - eval[i,j]),2) / (eval[i,j] + 0.5)\n         # Generate replicate data and compute fit stats for them\n         C.new[i,j] ~ dbin(p, N[i])\n         E.new[i,j] <- pow((C.new[i,j] - eval[i,j]),2) / (eval[i,j] + 0.5)\n   }\n}\n\n# Derived quantities\nlp <- logit(p)           # logit-scale detection\ntotalN <- sum(N[])       # Total abundance over all R sites\nfit <- sum(E[,])         # Fit stats actual data set\nfit.new <- sum(E.new[,]) # Fit stats ?ideal? data set\n}\n\",fill = TRUE)\nsink()\n\n\n# Define function to generate random initial values\nNst <- apply(C, 1, max) + 1\t# Can be important to give good inits for N\ninits <- function() list(N = Nst, alpha0 = runif(1, -1, 1), alpha1 = runif(1, -1, 1), p = runif(1))\n\n# Parameters monitored\nparams <- c(\"alpha0\", \"alpha1\", \"p\", \"lp\", \"totalN\", \"fit\", \"fit.new\")\n\n# MCMC settings\nni <- 10000\nnt <- 2\nnb <- 1000\nnc <- 3\n\n# Call WinBUGS from R (ART 0.7 min)\nlibrary(R2WinBUGS)\nout1 <- bugs(win.data, inits, params, \"Nmix.txt\", n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, debug = FALSE, working.directory = getwd())\n\n# Summarize posteriors\nprint(out1, 3)\n\n# Compare with MLEs from unmarked\nsummary(fm.nmix1 <- pcount(~1 ~vegHt, data=umf))\n\n# Call JAGS from R (ART 1.26 min)\nlibrary(\"R2jags\")\t\t# requires rjags\nsystem.time(out2 <- jags(win.data, inits, params, \"Nmix.txt\", n.chains = nc,\n   n.thin = nt, n.iter = ni, n.burnin = nb) )\ntraceplot(out2)\n\n# Summarize posteriors\nprint(out2, dig = 3)\n\npar(mar = c(5,5,4,5), cex.lab = 1.5)\nplot(vegHt, N, xlab=\"Vegetation height\", ylab=\"Abundance (N)\", las = 1)\nglm1.est <- coef(fm.glm1)\nplot(function(x) exp(-3 + 2*x), 1, 3, add=TRUE, lwd=3)\nlines(vegHt, predict(fm.nmix1, \"state\")[,1], lwd=3, col=\"blue\")\nlines(vegHt, exp(out1$mean$alpha0 + out1$mean$alpha1 * vegHt), lwd=3, col=\"green\", lty = \"dashed\")\nlines(vegHt, exp(out2$BUGSoutput$mean$alpha0 + out2$BUGSoutput$mean$alpha1 * vegHt), lwd=3, col=\"red\", lty = \"dotted\")\nlegend(1, max(N), c(\"Truth\", \"unmarked\", \"WinBUGS\", \"JAGS\"), col=c(\"black\", \"blue\", \"green\", \"red\"), lty=c(1, 1, 2, 3), lwd=3)\n\nplot(out1$sims.list$fit, out1$sims.list$fit.new, xlab=\"Actual data set\", ylab=\"?Perfect? data sets\", las = 1)\nabline(0,1, col = \"red\", lwd = 3)\n\n(bpv <- mean(out1$sims.list$fit.new > out1$sims.list$fit))\n# (bpv <- mean(out1$sims.list$fit.new > out1$sims.list$fit))\n[1] 0.2846667\n\n\n######################################################################################\ntmp[i] <- step(10-N[i])\nsum.critical <- sum(tmp[])\t# Number of pops with critical size\n\nplot(table(out1$sims.list$sum.critical), xlab=\"Number of threshold populations\", ylab=\"Frequency\")\nabline(v = 87.9, col = \"red\", lwd = 5)\n\n(metapop.extinction.risk <- mean(out1$sims.list$sum.critical>87))\n\n## 5.10. Exercises\n\n# Solution A:\nrange(mhbdata[,12:14], na.rm = TRUE)\nday.mean <- mean(as.matrix(mhbdata[,12:14]), na.rm = TRUE)\nday.sd <- sd(c(as.matrix(mhbdata[,12:14])), na.rm = TRUE)\noriginal.pred.day <- 15:110\npred.day <- (original.pred.day - day.mean) / day.sd\nnew<- data.frame(day=pred.day)\npred<-predict(fm31,type=\"det\",newdata=new,appendData=TRUE)\nhead(pred)\n\nplot(Predicted ~ original.pred.day, pred,type=\"l\",xlab=\"Date (1 = 1 April)\", ylab=\"Expected detection prob\",ylim=c(0,1), lwd = 2)\nlines(lower ~ original.pred.day, pred,type=\"l\",col=\"red\", lwd = 2)\nlines(upper ~ original.pred.day, pred,type=\"l\",col=\"red\", lwd = 2)",
    "created" : 1432530755437.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1556245035",
    "id" : "BFA4F4F6",
    "lastKnownWriteTime" : 1442332600,
    "path" : "~/CodigoR/Nancy/code/Nmix_model.R",
    "project_path" : "code/Nmix_model.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}